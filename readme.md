# Task 08 â€“ Bias Detection in LLM Data Narratives  
*A Complete, Reproducible Experiment on Framing, Demographic, Confirmation & Selection Bias in LLM Outputs*

---

## ğŸ“Œ Overview

This project investigates whether **Large Language Models (LLMs)** generate biased analytical narratives when analyzing **identical sports statistics** under **different prompt framings**.  

Using anonymized **Syracuse University Womenâ€™s Lacrosse data**, this experiment tests:

- **Framing Bias** (positive vs negative wording)  
- **Demographic Bias** (experience cues)  
- **Confirmation Bias** (hypothesis priming)  
- **Selection Bias** (which stats the LLM highlights)

The workflow includes prompt design, multi-sample LLM querying, structured logging, statistical analysis, factual validation, and final reporting.

This repository contains **all files, code, analysis, and reports** required for full reproducibility of Task 08.

---

## ğŸ“ Repository Structure

Task_08_Bias_Detection/
â”‚
â”œâ”€â”€ prompts/ # Prompt templates & variations
â”œâ”€â”€ results/ # Raw LLM outputs (JSON/CSV)
â”œâ”€â”€ analysis/ # Bias analysis outputs
â”‚ â”œâ”€â”€ analysis_summary.json
â”‚ â”œâ”€â”€ chi_square_results.json
â”‚ â””â”€â”€ fabrication_report.json
â”‚
â”œâ”€â”€ experiment_design.ipynb # Creates all prompt variations
â”œâ”€â”€ run_experiment.ipynb # Queries LLMs and logs responses
â”œâ”€â”€ analyze_bias.ipynb # Sentiment, mentions, chi-square
â”œâ”€â”€ validate_claims.ipynb # Checks factual accuracy
â”‚
â”œâ”€â”€ REPORT.md # Final written report
â”œâ”€â”€ Final_Report_Task08.docx # Formatted final report
â”œâ”€â”€ Initial_Planning_Report_Task08.docx
â”œâ”€â”€ Research_Task_08.docx # Assignment instructions
â”‚
â””â”€â”€ README.md # (This file)

---

## ğŸ¯ Objectives

This project followed the four-phase structure outlined in the Research Task 08 instructions:

1. **Experimental Design:**  
   - Build prompt pairs varying only by framing or demographic cues.  
   - Document expected â€œground truthâ€ based on sports statistics.

2. **Data Collection:**  
   - Query GPT models using each prompt.  
   - Gather multiple samples (3â€“5 responses per prompt).  
   - Log model version, timestamp, prompt, and narrative.

3. **Analysis:**  
   - Sentiment scoring  
   - Entity/keyword frequency  
   - Recommendation-type classification  
   - Chi-square statistical testing  
   - Narrative tone assessment  

4. **Validation:**  
   - Check if LLM claims match real data  
   - Detect hallucinations  
   - Compute fabrication rate  

---

## ğŸ§ª Methodology

### 1. Prompt Design
Created minimally different prompts that altered:

- Tone (â€œunderperformingâ€ vs â€œshowing potentialâ€)  
- Demographic context (â€œseniorâ€ vs â€œsophomoreâ€)  
- Hypothesis framing (â€œoffense is weakâ€)  
- Data emphasis (â€œfocus on turnoversâ€)

### 2. Response Collection
- LLMs used: **GPT-3.5 and GPT-4 (Oct 2025 builds)**  
- Standardized temperature and model parameters  
- Logged all data in `responses.csv`

### 3. Quantitative Analysis
- **Sentiment analysis:** VADER/TextBlob  
- **Keyword mentions:** Player A/B/C, stats, focus areas  
- **Recommendation type:** offense/defense/team/individual  
- **Statistical tests:** chi-square where applicable  

### 4. Factual Validation
Each narrative was compared with ground-truth lacrosse statistics to check:

- Numerical correctness  
- Claims about player performance  
- Hallucinated facts  

Fabrication rate: **0%**

---

## ğŸ“Š Key Findings

### âœ”ï¸ H1: Framing Bias â€” Mild  
Positive framing increased sentiment (+0.5) vs negative framing (0.0).  
Content stayed factually correct, but tone changed noticeably.

### âœ”ï¸ H2: Demographic Bias â€” None  
Adding experience level (senior/sophomore) did not change:
- Recommendations  
- Sentiment  
- Language tone  

### âœ”ï¸ H3: Confirmation Bias â€” None  
Priming the LLM with a hypothesis did not influence conclusions.

### âœ”ï¸ H4: Selection Bias â€” None  
The LLM did not preferentially highlight different statistics based on framing.

### âœ”ï¸ Fabrication Rate â€” 0%  
All LLM responses matched ground-truth performance data.  
No hallucinations were observed.

---

## ğŸ“˜ Bias Catalogue

| Bias Type | Observation | Severity |
|----------|-------------|----------|
| **Framing Bias** | Tone changes (â€œgrowth potentialâ€ vs â€œunderperformingâ€) | Mild |
| **Demographic Bias** | No change from experience cues | None |
| **Confirmation Bias** | LLM stayed neutral despite priming | None |
| **Selection Bias** | No preference for specific stats | None |
| **Factual Accuracy** | No hallucination detected | Strong |

---

## ğŸ“Œ Limitations

- Small sample size (10 responses)  
- Limited categorical diversity â†’ chi-square tests often inconclusive  
- Only GPT models examined deeply  
- Sentiment analysis tools may oversimplify tone  
- Narrative bias may require larger datasets to detect subtle patterns  

---

## ğŸ›  Tools Used

**Languages / Libraries**
- Python  
- Pandas, NumPy  
- SciPy (chi-square tests)  
- TextBlob, VADER  
- Matplotlib  

**Models**
- GPT-3.5 / GPT-4 (October 2025)  

**Environment**
- Jupyter notebooks  
- GitHub for version control  

---

## ğŸš€ How to Reproduce the Project

### 1. Install dependencies
